{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Recommendation Systems Exhibition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages used throughout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data import import_data \n",
    "from methods import utils, losses, matrix_factorization, AutoRec #, NeuMF, Caser"
   ]
  },
  {
   "source": [
    "## MovieLens Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "names = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    "data = pd.read_csv(\"data/u.data\", delimiter='\\t', names = names, engine = \"python\")\n",
    "num_users = data.user_id.unique().shape[0]\n",
    "num_items = data.item_id.unique().shape[0]"
   ]
  },
  {
   "source": [
    "## Matrix Factorization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the data\n",
    "num_users, num_items, train_iter, test_iter = import_data.split_and_load_ml100k(data,\n",
    "    num_users, num_items, test_ratio = 0.1, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluator\n",
    "def evaluator(net, test_iter):\n",
    "    rmse_list = []\n",
    "    for idx, (users, items, ratings) in enumerate(test_iter):\n",
    "        r_hat = net(users, items)\n",
    "        rmse_value = torch.sqrt(((r_hat - ratings)**2).mean())\n",
    "        rmse_list.append(float(rmse_value))\n",
    "\n",
    "    return np.mean(np.array(rmse_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "lr, num_epochs, wd = 0.002, 20, 1e-5\n",
    "\n",
    "mf_net = matrix_factorization.MF(30, num_users, num_items)\n",
    "optimizer = optim.Adam(mf_net.parameters(), lr = lr, weight_decay = wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate model\n",
    "rmse_list = []\n",
    "loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss, num_samples = 0, 0\n",
    "\n",
    "    # Train each batch\n",
    "    mf_net.train()\n",
    "    for i, (users, items, ratings) in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = mf_net(users, items)\n",
    "        output = ((predictions - ratings)**2).mean()\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += output\n",
    "        num_samples += users.shape[0]\n",
    "    \n",
    "    # Evaluate\n",
    "    mf_net.eval()\n",
    "    rmse = evaluator(mf_net, test_iter)\n",
    "    rmse_list.append(rmse)\n",
    "    loss_list.append(total_loss/num_samples)\n",
    "\n",
    "    print(f\"Epoch {epoch}:\\n\\tloss = {total_loss/num_samples}\\n\\trmse = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, ax = plt.subplots(2)\n",
    "fig.suptitle(f\"Matrix Factorization Test over {num_epochs} Epochs\")\n",
    "\n",
    "x_vals = list(range(1, num_epochs + 1))\n",
    "ax[0].plot(x_vals, rmse_list, label = \"RMSE\", color = \"blue\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticks(x_vals[::2])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(x_vals, loss_list, label = \"Loss\", color = \"green\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_xticks(x_vals[::2])\n",
    "ax[1].legend()"
   ]
  },
  {
   "source": [
    "## AutoRec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the data\n",
    "train_data, test_data = import_data.split_data_ml100k(data, num_users, num_items)\n",
    "\n",
    "_, _, _, train_inter_matrix = import_data.load_data_ml100k(train_data, num_users, num_items)\n",
    "_, _, _, test_inter_matrix = import_data.load_data_ml100k(test_data, num_users, num_items)\n",
    "\n",
    "train_inter_tensor = torch.from_numpy(train_inter_matrix).to(torch.float)\n",
    "test_inter_tensor = torch.from_numpy(test_inter_matrix).to(torch.float)\n",
    "\n",
    "train_iter = DataLoader(train_inter_tensor, shuffle = True, batch_size = 256,\n",
    "    num_workers = 4)\n",
    "test_iter = DataLoader(test_inter_tensor, shuffle = False, batch_size = 1024,\n",
    "    num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluator\n",
    "def evaluator(network, test_iter):\n",
    "    rmse_list = []\n",
    "    for idx, users_ratings in enumerate(test_iter):\n",
    "        recons = network(users_ratings)\n",
    "        rmse_value = torch.sqrt(\n",
    "            ((torch.sign(users_ratings) * recons - users_ratings)**2).mean())\n",
    "        rmse_list.append(float(rmse_value))\n",
    "\n",
    "    return np.mean(np.array(rmse_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "lr, num_epochs, wd = 0.002, 25, 1e-5\n",
    "\n",
    "autorec_net = AutoRec(500, num_users)\n",
    "optimizer = optim.Adam(autorec_net.parameters(), lr = lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate model\n",
    "rmse_list = []\n",
    "loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss, num_samples = 0, 0\n",
    "\n",
    "    # Train each batch\n",
    "    autorec_net.train()\n",
    "    for i, user_ratings in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = autorec_net(user_ratings)\n",
    "        output = ((predictions - user_ratings)**2).mean()\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += output\n",
    "        num_samples += user_ratings.shape[0]\n",
    "    \n",
    "    # Evaluate\n",
    "    autorec_net.eval()\n",
    "    rmse = evaluator(autorec_net, test_iter)\n",
    "    rmse_list.append(rmse)\n",
    "    loss_list.append(total_loss/num_samples)\n",
    "\n",
    "    print(f\"Epoch {epoch}:\\n\\tloss = {total_loss/num_samples}\\n\\trmse = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, ax = plt.subplots(2)\n",
    "fig.suptitle(f\"AutoRec Test over {num_epochs} Epochs\")\n",
    "\n",
    "x_vals = list(range(1, num_epochs + 1))\n",
    "ax[0].plot(x_vals, rmse_list, label = \"RMSE\", color = \"blue\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticks(x_vals[::2])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(x_vals, loss_list, label = \"Loss\", color = \"green\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_xticks(x_vals[::2])\n",
    "ax[1].legend()"
   ]
  },
  {
   "source": [
    "## NeuMF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "batch_size = 1024\n",
    "train_data, test_data = split_data_ml100k(data, num_users, num_items,'seq-aware')\n",
    "users_train, items_train, ratings_train, candidates = load_data_ml100k(\n",
    "    train_data, num_users, num_items, feedback=\"implicit\")\n",
    "users_test, items_test, ratings_test, test_iter = load_data_ml100k(\n",
    "    test_data, num_users, num_items, feedback=\"implicit\")\n",
    "train_iter = gluon.data.DataLoader(\n",
    "    PRDataset(users_train, items_train, candidates, num_items ), batch_size,\n",
    "    True, last_batch=\"rollover\", num_workers=get_dataloader_workers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize model\n",
    "devices = try_all_gpus()\n",
    "net = NeuMF(10, num_users, num_items, nums_hiddens=[10, 10, 10])\n",
    "net.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "lr, num_epochs, wd, optimizer = 0.01, 8, 1e-5, 'adam'\n",
    "loss = BPRLoss()\n",
    "hit_rate_list_neu, auc_list_neu = train_ranking(net, train_iter, test_iter, loss, trainer,\n",
    "                          None, num_users, num_items, num_epochs, devices, evaluate_ranking, candidates)"
   ]
  },
  {
   "source": [
    "## Caser"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "TARGET_NUM, L, batch_size = 1, 5, 4096\n",
    "train_data, test_data = split_data_ml100k(data, num_users, num_items,\n",
    "                                              'seq-aware')\n",
    "users_train, items_train, ratings_train, candidates = load_data_ml100k(\n",
    "    train_data, num_users, num_items, feedback=\"implicit\")\n",
    "users_test, items_test, ratings_test, test_iter = load_data_ml100k(\n",
    "    test_data, num_users, num_items, feedback=\"implicit\")\n",
    "train_seq_data = SeqDataset(users_train, items_train, L, num_users,\n",
    "                            num_items, candidates)\n",
    "train_iter = gluon.data.DataLoader(train_seq_data, batch_size, True,\n",
    "                                   last_batch=\"rollover\",\n",
    "                                   num_workers=get_dataloader_workers())\n",
    "test_seq_iter = train_seq_data.test_seq\n",
    "train_seq_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = try_all_gpus()\n",
    "net = Caser(10, num_users, num_items, L)\n",
    "net.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))\n",
    "lr, num_epochs, wd, optimizer = 0.04, 8, 1e-5, 'adam'\n",
    "loss = BPRLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer,\n",
    "                        {\"learning_rate\": lr, 'wd': wd})\n",
    "\n",
    "hit_rate_list_caser, auc_list_caser = train_ranking(net, train_iter, test_iter, loss, trainer, test_seq_iter, num_users, num_items, num_epochs, devices, evaluate_ranking, candidates, eval_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}